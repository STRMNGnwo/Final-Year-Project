{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/STRMNGnwo/Final-Year-Project/blob/main/FYP_UNETmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM0-aLwOs3eh"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "#files.upload()\n",
        "\n",
        "#g drive file path to total dataset /content/drive/MyDrive/FYP-Aortic_Dissection_Segmentation/datasets\n",
        "#g drive file path to split dataset /content/drive/MyDrive/FYP-Aortic_Dissection_Segmentation/Split_Patients\n",
        "#g drive file path to split dataset /content/drive/MyDrive/FYP-Aortic_Dissection_Segmentation/\n",
        "drive.mount(\"/content/drive/\") \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdCHmab5stpj"
      },
      "outputs": [],
      "source": [
        "#importing the required modules to create the U-Net model\n",
        "import torch, torchvision\n",
        "from torch.nn import Module\n",
        "from torch.nn import Conv2d #The convolutional layer used in the U-Net architecture\n",
        "from torch.nn import ReLU #activation function\n",
        "from torch.nn import BatchNorm1d, BatchNorm2d\n",
        "from torch.nn import Sequential\n",
        "from torch.nn import functional as f #to make the fully connected layer\n",
        "from torch.nn import ConvTranspose2d\n",
        "from torch.nn import MaxPool2d #max pooling layer that is implemented after Conv2d layers\n",
        "from torchvision import transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tqdm\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, '/content/drive/MyDrive/FYP-Aortic_Dissection_Segmentation/')\n",
        "\n",
        "#from CustomDataset import SegmentationCTADataset\n",
        "from CustomDataset import CTADataset\n",
        "#from split_patients import remove_DS_Store, split_patients, copy_collate_datasets\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device(\"cuda\")\n",
        "  torch.cuda.get_device_name(0)\n",
        "  print(\"GPU\")\n",
        "\n",
        "else:\n",
        "  device=\"cpu\"\n",
        "  print(\"CPU\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUu3SuR5stpz"
      },
      "source": [
        "Importing the datasst data, which has been split into train,test and val sets by unzipping the prepared dataset file that is found in my google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFXz_lKvstpz"
      },
      "outputs": [],
      "source": [
        "\n",
        "!unzip /content/drive/MyDrive/FYP-Aortic_Dissection_Segmentation/Prepared_Dataset.zip\n",
        "\n",
        "#unzip /content/drive/MyDrive/FYP-Aortic_Dissection_Segmentation/SampleTrain.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52R9wpR6stpp"
      },
      "source": [
        "Sliding Window CNNs involve dividing an input image into \"patches\" of pixels, passing each pixel into a CNN, and assigning a class label to the pixel. Their main benefit is that they can localize, and they can also result in more training data being available as a result of the patches being made. However they are very slow and computationally expensive.\n",
        "\n",
        "Localization -> assigning a class label to a region of pixels on a pixel-by-pixel basis.\n",
        "\n",
        "Trade-off between localization accuracy and the context available to the CNN-> exists due to varying patch sizes. Large Patch sizes allow for greater context, but lower localization accuracy and vice-versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-fubXaSstpr"
      },
      "source": [
        "The U-NET Architecture:\n",
        "\n",
        "Consists of a contracting path that identifies context (descending/encoding) and a symmetrical expanding path that enables precise localization\n",
        "of contours. U-NET is popular for medical image segmentation and is proven to be better than sliding window CNNs.\n",
        "\n",
        "The Descending section:\n",
        "\n",
        "The descending section of the architecture consists of convolutional layers that are followed by max-pooling layers . \n",
        "This pattern (Conv,Conv,Max-Pool) repeats. \n",
        "The input image is down-sampled (image size reduces but the number of channels increases).\n",
        "After convolutions ->the number of channels seems to increase.\n",
        "After max-pooling -> the image size decreases.\n",
        "\n",
        "---------------------------\n",
        "The Ascending section:\n",
        "\n",
        "Consists of an expanding path, made up of convolutional layers and transposed convolutional layers(that upsample, ie increasing image size).\n",
        "The Conv. layers in this section seem to reduce the number of channels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9-LhSgmstpt"
      },
      "source": [
        "Notes from paper (3D UNET was used for segmentation): https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7817629/\n",
        "\n",
        "The down-sampling path had a filter size of 3 × 3 × 32 and stride 2 in each convolution layer. \n",
        "\n",
        "All convolution layers were processed with batch normalization, Rectified Linear Units, and same-padding. T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiJ5SEZCstpt"
      },
      "outputs": [],
      "source": [
        "#Building block of both Encoder section and Decoder section.\n",
        "#At a high level: Encoder= Block +downsampling (max_pooling), Decoder= upsampling+ skip connection concatenation + Block\n",
        "# Block -> Double Convolutional layers, with Batch Normalisation and ReLU activation functions between them.\n",
        "class Block(Module):\n",
        "\n",
        "    def __init__(self, in_channels,out_channels):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.conv=Sequential(\n",
        "            #Conv1\n",
        "            #in_channels,out_channels,filter_size(kernel),stride,padding(same padding=1), bias\n",
        "            Conv2d(in_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False), # bias is false because batch normalisation is used in the next step and it cancels out bias\n",
        "            #NOTE: Would I need BatchNorm1D or BatchNorm2D?\n",
        "            \n",
        "            BatchNorm2d(out_channels),\n",
        "            ReLU(inplace=True),\n",
        "\n",
        "            #Conv2\n",
        "            Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False), #bias=False because batch normalisation is used in the next step and it cancels out bias\n",
        "            #NOTE: Would I need BatchNorm1D or BatchNorm2D?\n",
        "            \n",
        "            BatchNorm2d(out_channels),\n",
        "            ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TQ6Lemastpv"
      },
      "outputs": [],
      "source": [
        "#the descending section made of conv.layers and max-pool layers.\n",
        "#The Encoder consists of multiple \"blocks\"  which consist of down-sampling and convolutional layers. \n",
        "#The Encoder reduces the spatial dimension and obtains information about the mask area/region of interest.\n",
        "class Encoder(Module): \n",
        "\n",
        "    def  __init__(self,encoding_channels=(1,64,128,256,512)): #the channel values should probably be modified based on the paper.. 3 assumes rgb but images I have are grey scale\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.channels=encoding_channels\n",
        "        self.encoding_blocks=torch.nn.ModuleList()\n",
        "\n",
        "        for i in range(0,len(self.channels)-1):\n",
        "            in_channel=self.channels[i]\n",
        "            out_channel=self.channels[i+1]\n",
        "            self.encoding_blocks.append(Block(in_channel,out_channel))\n",
        "\n",
        "        self.pool= MaxPool2d(kernel_size=2,stride=2) #reduces spatial dimensions by a factor of 2 each time it is called\n",
        "\n",
        "    \n",
        "    def forward(self,x):\n",
        "        #intermediary results between blocks are stored here\n",
        "        block_outputs=[]\n",
        "\n",
        "        for block in self.encoding_blocks:\n",
        "             \n",
        "            #print(\"Data before pooling and before convolutions\",x.shape)\n",
        "            x=block(x) # the block's forward method is implicitly called.\n",
        "\n",
        "            #print(\"Data before pooling and after convolutions\",x.shape)\n",
        "\n",
        "            #adding the block output to the list\n",
        "            block_outputs.append(x) \n",
        "\n",
        "            #sending the output of the block to the max pooling layer\n",
        "            x=self.pool(x)\n",
        "            #print(\"Data after pooling\",x.shape)\n",
        "\n",
        "    #return the final form of the data and the intermediary results (skip-connections used in the Decoder section, in the U-NET architecture paper)        \n",
        "        return [x,block_outputs] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCKt9ZaPstpw"
      },
      "outputs": [],
      "source": [
        "#The Decoder class has Decoder blocks to take in input data and skip connections, perform upsampling, concatenation of skip connection to data and Conv2D.\n",
        "#The Decoder upsamples the localized information and provides context.\n",
        "class Decoder(Module):\n",
        "    \n",
        "    def __init__(self,decoding_channels=(512,256,128,64,2)): #2 is the final one because of 2 classes-> TL and FL\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.channels=decoding_channels\n",
        "        self.decoding_blocks=torch.nn.ModuleList()\n",
        "        \n",
        "        #upsampling going to use transpose convolutional layers. Can also use bilinear + convolution\n",
        "        self.upconvolutions=torch.nn.ModuleList()\n",
        "\n",
        "        #making the up-sampling layers and the decoding blocks\n",
        "        for i in range(0,len(decoding_channels)-1):\n",
        "            in_channel=self.channels[i]\n",
        "            out_channel=self.channels[i+1]\n",
        "            self.decoding_blocks.append(Block(in_channel,out_channel))\n",
        "\n",
        "            #NOTE due to the skip connections having to be added, would in_channel(first param below)  have to be *2\n",
        "            self.upconvolutions.append(ConvTranspose2d(self.channels[i],self.channels[i+1],kernel_size=2,stride=2))\n",
        "\n",
        "        \n",
        "    def forward(self,x,encoder_intermediary_outputs):#encoder_intermediary_outputs are the list of skip connections from the encoder.\n",
        "        \n",
        "        #print(\"\\n--------------Decoder------------------------\")\n",
        "        #print(\"Data in decoder\")\n",
        "\n",
        "        # for skipconnection in encoder_intermediary_outputs:\n",
        "        #     print(\"Shape of skip connection: \",skipconnection.shape)\n",
        "        \n",
        "        for i in range (len(self.upconvolutions)):\n",
        "\n",
        "            #print(\"Data shape before upsampling\",x.shape)\n",
        "\n",
        "            #using a transpose convolution to upsample the data (should )\n",
        "            x=self.upconvolutions[i](x)\n",
        "\n",
        "            #print(\"Data shape after upsampling\",x.shape)\n",
        "            \n",
        "            #concatenating an intermediary output from encoder section to the data\n",
        "            cropped_encoder_output=self.crop(encoder_intermediary_outputs[i],x)\n",
        "            x=torch.cat([x,cropped_encoder_output],dim=1)\n",
        "\n",
        "            #print(\"Data shape after concatenation with skip connection\",x.shape)\n",
        "\n",
        "            #sending the concatenated upsampled data through a decoding block\n",
        "            x= self.decoding_blocks[i](x)\n",
        "            #print(\"Data shape after upsampling, concatenation and convolution\",x.shape)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def crop(self, encoding_intermediary_output,x):\n",
        "        \n",
        "        #(_,_,H,W)=x.shape\n",
        "        (_,_,H,W)=x.shape\n",
        "        cropped_intermediary_output=torchvision.transforms.CenterCrop([H, W])(encoding_intermediary_output)\n",
        "\n",
        "        return cropped_intermediary_output\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUsHvNxb-Zf5"
      },
      "outputs": [],
      "source": [
        "class Bottleneck(Module):\n",
        "\n",
        "  def __init__(self, in_channels=None, out_channels=None):\n",
        "\n",
        "    super().__init__()\n",
        "    self.in_channels=in_channels\n",
        "    self.out_channels=out_channels\n",
        "\n",
        "    #bottleneck_block=Block(in_channels=encoder_intermediaries[::-1],out_channels=(encoder_intermediaries[::-1]*2))\n",
        "    self.block=Block(self.in_channels,self.out_channels)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeWV-KJBstpx"
      },
      "outputs": [],
      "source": [
        "#Creating the U-NET class\n",
        "\n",
        "#U-NET architecture:\n",
        "\n",
        "\"\"\" \n",
        "Consists of a contracting path that identifies context (descending/encoding) and a symmetrical expanding path that enables precise localization\n",
        "of contours. U-NET is popular for medical image segmentation and is proven to be better than sliding window CNNs.\n",
        "\n",
        "Localization -> assigning a class label to a region of pixels on a pixel-by-pixel basis.\n",
        "\"\"\"\n",
        "\n",
        "class UNet(Module):\n",
        "\n",
        "    def __init__(self,encoding_channels=(1,64,128),decoding_channels=(256,128,64),seg_classes=3, retainDim=True, outSize=(128, 128)): \n",
        "        \n",
        "        #seg_classes is the number of classes that a pixel can belong to (2 being- TL and FL)\n",
        "        #seg_classes is also the number of channels expected by the final conv layer which generates the map.\n",
        "\n",
        "        #retainDim signifies if the original output dimension should be maintained or not when producing the map.\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        #Encoder and Decoder are classes. Encoder is the descending section and the Decoder is the ascending of \"U\"\n",
        "        self.encoder=Encoder(encoding_channels)\n",
        "        self.bottleneck_block= Bottleneck(encoding_channels[-1],(encoding_channels[-1]*2) )\n",
        "        self.decoder=Decoder(decoding_channels)\n",
        "\n",
        "        self.retainDim=retainDim\n",
        "        self.seg_classes=seg_classes\n",
        "        self.outSize=outSize\n",
        "\n",
        "        #defining the last single convolutional layer, which would output the segmentation map\n",
        "        #in_channel,out_channel,kernel_size\n",
        "        self.head=Conv2d(decoding_channels[-1],seg_classes,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        #data's entry point into the model\n",
        "\n",
        "        #get intermediary output from encoder, as calling it implicity calls its forward method\n",
        "        encoder_results=self.encoder(x)\n",
        "        #obtain the skip connections\n",
        "        encoder_intermediaries=encoder_results[1]\n",
        "        #obtain the image that needs to be sent into the bottleneck block (\"between the Descending and Ascending sections\")\n",
        "        final_data_from_encoder=encoder_results[0]\n",
        "\n",
        "\n",
        "        #print(\"Encoder output shape:\",final_data_from_encoder.shape)\n",
        "\n",
        "        # the \"bottleneck\" layer-> layer between encoder and decoder, which has a double conv2d.\n",
        "\n",
        "        #passing in the final image output from the encoder into the bottleneck block (does 2 Conv2D convolutions)\n",
        "        bottleneck_output=self.bottleneck_block(final_data_from_encoder)\n",
        "\n",
        "        #passing in the encoder output in reverse(latest output to oldest output) as the data to its forward function\n",
        "        #and also passing in the encoder outputs to decoder\n",
        "        \n",
        "        #decoder_output=self.decoder(encoder_intermediaries[::-1][0],encoder_intermediaries[::-1][1:])\n",
        "\n",
        "\n",
        "        #print(\"Bottleneck block output shape:\",bottleneck_output.shape)\n",
        "\n",
        "        decoder_output=self.decoder(bottleneck_output,encoder_intermediaries[::-1])\n",
        "\n",
        "        map = self.head(decoder_output)\n",
        "\t\t\n",
        "        if self.retainDim:\n",
        "            map = torch.nn.functional.interpolate(map, self.outSize)\n",
        "\t\t\n",
        "        #print(\"The type of the model output is :\",type(map))\n",
        "        #print(\"The shape of the segmentation map is: \",map.shape)\n",
        "        \n",
        "        # return the segmentation map\n",
        "        return map\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y4n_Lvdstpy"
      },
      "outputs": [],
      "source": [
        "def visualiser(image):\n",
        "\n",
        "    print(type(image))\n",
        "    if(type(image)==torch.Tensor):\n",
        "        print(\"tensor\")\n",
        "        transform=transforms.ToPILImage()\n",
        "        display=transform(image)\n",
        "        display.show()  \n",
        "\n",
        "    elif(type(image)==Image.Image):\n",
        "        print(\"image\")\n",
        "        image.show()\n",
        "\n",
        "from numpy import ndarray\n",
        "def visualiser_sample(image):\n",
        "  print(type(image))\n",
        "\n",
        "  if(type(image)==torch.Tensor):\n",
        "    print(\"Image is in a tensor format\")\n",
        "    plt.imshow(image.permute(1, 2, 0))\n",
        "    #plt.imshow(image)\n",
        "    #plt.imshow((image* 255).astype(np.uint8))\n",
        "    plt.show()\n",
        "\n",
        "  if(type(image)==np.ndarray):\n",
        "    print(\"Image is a numpy array\")\n",
        "    image=torch.from_numpy(image)\n",
        "    plt.imshow(image)\n",
        "    plt.imshow((image* 255).astype(np.uint8))\n",
        "    plt.show()\n",
        "\n",
        "def visualiser_colab(image):\n",
        "  print(type(image))\n",
        "  print(\"Image shape in visualiser_colab:\",image.shape)\n",
        "\n",
        "  if(type(image)==torch.Tensor):\n",
        "    print(\"Image is in a tensor format\")\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "  if(type(image)==np.ndarray):\n",
        "    print(\"Image is a numpy array\")\n",
        "    image=torch.from_numpy(image)\n",
        "    #plt.imshow(image.permute(1, 2, 0))\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMBjGm_rlq5B"
      },
      "outputs": [],
      "source": [
        "# from matplotlib import image\n",
        "# from matplotlib import pyplot\n",
        "\n",
        "# images=image.imread(\"/content/Prepared_Dataset/train/images/patient1-slice150.jpg\")\n",
        "# mask=image.imread(\"/content/Prepared_Dataset/train/masks/patient1-mask-slice150.jpg\")\n",
        "\n",
        "# pyplot.imshow(images)\n",
        "# pyplot.show()\n",
        "# pyplot.imshow(mask)\n",
        "# pyplot.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EINbT3kstp0"
      },
      "outputs": [],
      "source": [
        "from matplotlib import image\n",
        "from matplotlib import pyplot\n",
        "train_path=\"/content/Prepared_Dataset/train\"\n",
        "val_path=\"/content/Prepared_Dataset/val\"\n",
        "test_path=\"/content/Prepared_Dataset/test\"\n",
        "\n",
        "sample_path=\"/content/SampleTrain\"\n",
        "\n",
        "#specifying transforms-> increasing contrast in the image and converting it to a Tensor\n",
        "train_transforms=transforms.Compose([transforms.Resize((128,128)),transforms.ToTensor()])\n",
        "val_transforms=transforms.Compose([transforms.ToTensor()])\n",
        "test_transforms=transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "#Setting up Custom Datasets and converting appropriate images to Tensors.\n",
        "\n",
        "train_dataset= CTADataset(os.path.join(train_path,\"images\"),os.path.join(train_path,\"masks\"),transform=train_transforms)\n",
        "val_dataset= CTADataset(os.path.join(val_path,\"images\"),os.path.join(val_path,\"masks\"),transform=val_transforms)\n",
        "test_dataset=CTADataset(os.path.join(test_path,\"images\"),os.path.join(test_path,\"masks\"),transform=test_transforms)\n",
        "\n",
        "#sample_dataset=CTADataset(os.path.join(sample_path,\"images\"),os.path.join(sample_path,\"masks\"),transform=train_transforms)\n",
        "\n",
        "#getting a sample image and mask from an arbitrary patient and displaying it.\n",
        "# target_map is the semantically segmented ground truth to be used in loss func\n",
        "train_sampleimage,train_samplemask,imagePath,maskPath,target_map=train_dataset.__getitem__(980)\n",
        "\n",
        "print(train_sampleimage.shape)\n",
        "# image_np= np.asarray(train_sampleimage)\n",
        "\n",
        "# print(image_np.size)\n",
        "print(target_map.shape)\n",
        "print(target_map.size)\n",
        "\n",
        "\n",
        "print(\"Length of sample dataset: \",train_dataset.__len__())\n",
        "# visualiser_sample(train_sampleimage)\n",
        "# visualiser_sample(train_samplemask)\n",
        "\n",
        "pyplot.imshow(train_samplemask.permute(1,2,0))\n",
        "pyplot.show()\n",
        "\n",
        "#for pyplot to show image, it must be in format: width,height. target map-> channel, width, height\n",
        "pyplot.imshow(np.squeeze(target_map)) #squeeze removes the channel dimension as its just 1\n",
        "\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLvJN99Qstp1"
      },
      "source": [
        "Creation of DataLoaders for the train, test and val datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7VKVlorstp1",
        "outputId": "00919f96-ed86-4dc9-fbce-782b8f31692e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "#making dataloaders for the train, test and val sets.\n",
        "\n",
        "train_loader= DataLoader(train_dataset,batch_size=16,shuffle=False,drop_last=True)\n",
        "\n",
        "print(train_loader.drop_last)\n",
        "# val_loader= DataLoader(val_dataset,batch_size=32,shuffle=False)\n",
        "# test_loader= DataLoader(test_dataset,batch_size=32,shuffle=False)\n",
        "\n",
        "# print(sample_dataset.__len__())\n",
        "\n",
        "#sample_loader=DataLoader(sample_dataset,batch_size=16,shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARxKoJCgstp2"
      },
      "source": [
        "Instantiating the UNET implementation model\n",
        "\n",
        "Encoder:\n",
        "Down-sample -> Max-pooling to reduce the size of the image.\n",
        "Conv2D-> To increase the number of channels.\n",
        "\n",
        "Decoder: \n",
        "Up-Sample -> To increase the image size\n",
        "Conv2D-> To decrease the number of channles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDrfN2pJ0eb5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi3YSs3Ostp2"
      },
      "outputs": [],
      "source": [
        "#Instantiate the Block, Encoder,Decoder and UNET classes. Pass data into the UNET Class (model's data entrypoint)\n",
        "print(\"The device being used is: \",device)\n",
        "\n",
        "seg_map_height=128\n",
        "seg_map_width=128\n",
        "\n",
        "#encoding_channels=(3,64,128),decoding_channels=(256,128,64,2)\n",
        "UNET_Model= UNet(encoding_channels=(3,64,128,256,512),decoding_channels=(1024,512,256,128,64),outSize=(seg_map_height,seg_map_width),seg_classes=3)\n",
        "UNET_Model.to(device)\n",
        "\n",
        "print(\"Encoder channels: \",UNET_Model.encoder.channels)\n",
        "print(\"Decoder Channels: \", UNET_Model.decoder.channels)\n",
        "\n",
        "# loss_func=torch.nn.BCEWithLogitsLoss()\n",
        "# # train_sampleimage,train_samplemask=train_dataset.__getitem__(10)\n",
        "# for (batch_index,(image,mask,imagePath,maskPath)) in enumerate(sample_loader):  \n",
        "#   predictions=UNET_Model(image)\n",
        "#   print(\"Image used: \",imagePath)\n",
        "#   print(\"Mask used: \",maskPath)\n",
        "#   print(\"Loss: \",loss_func(predictions,mask))\n",
        "\n",
        "# #the torch.squeeze is done to remove the batch_size dimension from the image, which is usually 1 [batch_size,channels,height,width].\n",
        "\n",
        "# print(\"The data type of the prediction is : \",type(predictions))\n",
        "# predictions=torch.squeeze(predictions)\n",
        "# visualiser_colab(predictions.detach().numpy())\n",
        "# mask=torch.squeeze(mask)\n",
        "# visualiser_colab(mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtnAemTrEeDt"
      },
      "source": [
        "Defining the model's hyper-parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X16dYG6stpo"
      },
      "outputs": [],
      "source": [
        "#defining and initialising model hyper-parameters\n",
        "\n",
        "initial_learning_rate=0.0001\n",
        "\n",
        "#Optimizer function -> Adam\n",
        "optimizer_function= torch.optim.Adam(params=UNET_Model.parameters(),lr=initial_learning_rate)\n",
        "\n",
        "batch_size=16\n",
        "\n",
        "num_epochs=1\n",
        "\n",
        "training_loss_list=[]\n",
        "test_loss_list=[]\n",
        "\n",
        "#loss_func= binary_cross_entropy\n",
        "#loss_function=torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "#loss_func= cross_entropy (as its a 3 class pixel classification problem)\n",
        "\n",
        "'''\n",
        "for CrossEntropyLoss to work, expected mask/target should be either \n",
        "Class indices in the range [0,C) where C is the number of classes or\n",
        "Probabilities for each class\n",
        "'''\n",
        "\n",
        "#What I think is happening: \n",
        "'''\n",
        "Mask data does not contain classes assigned to pixels. It is simply rgb pixel values.\n",
        "\n",
        "image and mask values have been normalized from (0 to 255) to (0,1)\n",
        "so what if CrossEntropyLoss is assuming them to be probability values and as all the values are between\n",
        "0 and 1 (like 0.2, 0.3, 0.5) it is flooring them to be class 0?.\n",
        "\n",
        "'''\n",
        "\n",
        "loss_function=torch.nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn-d-EXgstp2"
      },
      "source": [
        "Training the model on the training samples. \n",
        "Calculating loss/error using the loss function. \n",
        "Backpropagating the error through the model, to readjust weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXu_CP2Fstp3"
      },
      "outputs": [],
      "source": [
        "total_loss=0\n",
        "\n",
        "#Training\n",
        "\n",
        "#epoch -> entire run of a CNN through an entire training dataset.\n",
        "#enumeration of a data loader-> passing batches of data to train the model, backpropogate error and optimise weights\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  #setting the model mode to train\n",
        "  UNET_Model.train()\n",
        "\n",
        "  images_list=[]\n",
        "  masks_list=[]\n",
        "  images_path_list=[]\n",
        "  masks_path_list=[]\n",
        "  predictions_list=[]\n",
        "  target_maps_list=[]\n",
        "\n",
        "  #looping through batches of data made by the training DataLoader.\n",
        "  for (batch_index,(image,mask,imagePath,maskPath,target_map)) in tqdm.tqdm(enumerate(train_loader),total=len(train_loader)):\n",
        "    \n",
        "    #switching the data over to the GPU\n",
        "    image=image.to(device)\n",
        "    #somehow target_map becomes a tensor from a np array. values still remain the same (0,1,2)\n",
        "    #shape of target-> batch, 1(channel), width, height\n",
        "\n",
        "    target_map= torch.squeeze(target_map) #removing the 1 channel this is because CELoss expects form->[N, H, W]\n",
        "    #print(\"target_map dimension\",target_map.shape)\n",
        "    target_map=target_map.to(dtype=torch.long)\n",
        "    target_map=target_map.to(device) \n",
        "\n",
        "    if i==num_epochs-1:\n",
        "      #adding images and masks in this list to batch\n",
        "      images_list.append(image)\n",
        "      masks_list.append(mask)\n",
        "      images_path_list.append(imagePath)\n",
        "      masks_path_list.append(maskPath)\n",
        "      target_maps_list.append(target_map)\n",
        "\n",
        "    predictions=UNET_Model(image)\n",
        "\n",
        "    predictions_list.append(predictions)\n",
        "\n",
        "    #print(\"The data type of the prediction is : \",type(predictions))\n",
        "\n",
        "    #Calculating loss\n",
        "    \n",
        "    loss=loss_function(predictions,target_map)\n",
        "    training_loss_list.append(loss.item())\n",
        "    #total_loss+=loss.item()\n",
        "\n",
        "    #zeroing out accumulated gradients as weights have already been modified according to them during the previous run.\n",
        "    optimizer_function.zero_grad()\n",
        "\n",
        "    #Back propogation:\n",
        "    loss.backward()\n",
        "\n",
        "    #updating weights\n",
        "    optimizer_function.step()\n",
        "\n",
        "\n",
        "print(\"Ended training phase\")\n",
        "print(\"Training losses are as follows: \",training_loss_list)\n",
        "\n",
        "##NOTE: Images are represented during training in the format: [batchsize,channels,height,width]\n",
        "\n",
        "#displaying the predicted mask and the actual mask\n",
        "print(\"The data type of the prediction is : \",type(predictions))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzhPQjGJinWy"
      },
      "outputs": [],
      "source": [
        "#array[0][:][:][:]\n",
        "print(\"Predictions stored: \",len(predictions_list))\n",
        "print(\"Predictions(from model) shape:\",predictions.shape)\n",
        "\n",
        "print(\"Images stored\",len(images_list))\n",
        "print(\"Masks stored\",len(masks_list))\n",
        "print(\"Target maps stored\", len(target_maps_list))\n",
        "\n",
        "print(\"Image paths stored\",len(images_path_list))\n",
        "print(\"Mask paths stored\",len(masks_path_list))\n",
        "\n",
        "#getting a certain index of predictions,images and masks.\n",
        "list_index=378\n",
        "image=images_list[list_index]\n",
        "mask= masks_list[list_index]\n",
        "predictions=predictions_list[list_index]\n",
        "target_map=target_maps_list[list_index]\n",
        "\n",
        "#to convert tensor to numpy array to plot, switch it from gpu to cpu\n",
        "mask=mask.cpu()\n",
        "predictions=predictions.cpu()\n",
        "image=image.cpu()\n",
        "\n",
        "print(\"Shape of the Image from list : \", image.shape)\n",
        "print(\"Shape of the expected mask from list: \", mask.shape)\n",
        "print(\"Shape of the predictions from list: \", predictions.shape)\n",
        "print(\"Shape of the target_maps from list: \",target_map.shape)\n",
        "print(\"Image type: \",type(image))\n",
        "\n",
        "batch=batch_size-1\n",
        "\n",
        "#taking a single image from the batch of images, \n",
        "#permuting it to display it and then displaying the input image using pyplot's imshow function.\n",
        "print(\"Input image\")\n",
        "image=image[batch][:][:][:]\n",
        "image=image.permute(1,2,0)\n",
        "plt.imshow(image.detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "#reading in mask 1, from the batch of 32. \n",
        "#Also permuting it, so shape becomes [height,width,channel]\n",
        "print(\"Expected mask\")\n",
        "mask=mask[batch][:][:][:]\n",
        "mask=mask.permute(1,2,0)\n",
        "plt.imshow(mask.detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Ground truth target mask passed into loss func:\")\n",
        "target_map=target_map.cpu()\n",
        "visualiser_colab(target_map[batch][:][:])\n",
        "print(\"The Model output mask\")\n",
        "\n",
        "print(\"Prediction type from list : \",type(predictions))\n",
        "\n",
        "#reading in prediction 1, from the batch of 32\n",
        "#also permuting it so shape becomes [height,width,channel]\n",
        "predictions=predictions[batch][:][:][:]\n",
        "print(\"predictions shape before being displayed: \",predictions.shape)\n",
        "predictions=predictions.permute(1,2,0) #shape becomes 128,128,3\n",
        "print(\"Predictions shape after permuting it:\",predictions.shape)\n",
        "# predictions=transf(predictions)\n",
        "visualiser_colab(predictions.detach().numpy())\n",
        "\n",
        "\n",
        "\n",
        "# #printing out the prediction as a numpy array with 0,1 intensity.\n",
        "# imageDisp=np.clip(predictions.detach().numpy(), 0, 1)\n",
        "# plt.imshow(imageDisp)\n",
        "# plt.show()\n",
        "\n",
        "# #printing out the prediction as a numpy array, and trying to remove the clipping warning\n",
        "# imageDisp=predictions.detach().numpy()\n",
        "# plt.imshow((imageDisp* 255).astype(np.uint8))\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSamggw9rS0A"
      },
      "outputs": [],
      "source": [
        "#function to calculate dice score\n",
        "\n",
        "\n",
        "\n",
        "#calculating dice score for predictions stored in the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FI_IKUwxoBL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXaq4TNUfbZj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-aprHfo9qUM"
      },
      "outputs": [],
      "source": [
        "#SAVING THE MODEL\n",
        "path=\"/content/drive/MyDrive/FYP-Aortic_Dissection_Segmentation/saved_model-3pc4epoch\"\n",
        "torch.save(UNET_Model,path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "practiceInitial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "f49d0fc0e1a62c2e2d90b2c478c493e6e1789ab5d4aba8233aecb17a3c2190e7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}